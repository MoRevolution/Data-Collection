{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f) \n",
    "    return (data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Word extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to extract keywords\n",
    "def extract_keywords(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "    # Tag the tokens with their part of speech\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    # Define a function to convert part of speech tags to WordNet compatible tags\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    # Lemmatize the tokens using WordNet\n",
    "    lemmatizer = wordnet.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    # Identify named entities using NLTK's named entity recognition (NER) module\n",
    "    named_entities = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "\n",
    "    # Extract the most common lemmas and named entities\n",
    "    keywords = [token for token, count in Counter(lemmatized_tokens + [chunk[0] for chunk in named_entities if hasattr(chunk, 'label')]).most_common(10)]\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download using nltk.download('punkt') if you get an nltk error\n",
    "all_keywords = []\n",
    "for index, story in enumerate(corpus):\n",
    "    keywords = extract_keywords(story[index])\n",
    "    all_keywords.append(keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the stop words for English\n",
    "spacy_stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# choose the set of english stopwords\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess the input corpus\n",
    "def preprocess_text(text):\n",
    "    # tokenize a story within a corpus \n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # remove alphanumeric characters and stop words\n",
    "    cleaned_tokens = [token for token in tokens if token.isalpha() and token not in spacy_stop_words]\n",
    "    return cleaned_tokens\n",
    "\n",
    "cleaned_corpus = [preprocess_text(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorize using TfidfVectorizer which combines counting and noralized weighting\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_keywords(tfidf_scores, feature_names, top_n):\n",
    "    sorted_scores = np.argsort(tfidf_scores)[::-1]\n",
    "    top_keywords = [feature_names[i] for i in sorted_scores[:top_n]]\n",
    "    return top_keywords\n",
    "\n",
    "for i, doc in enumerate(corpus):\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "    top_keywords = get_top_keywords(tfidf_scores, feature_names, 10)\n",
    "    print(f\"Document {i+1} top keywords: {top_keywords}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimental "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the keywords and phrases that are likely to appear in relevant stories\n",
    "relevant_keywords = ['low socio-economic background', 'higher education', 'difficulties', 'overcome']\n",
    "\n",
    "# Define the path to the file containing the stories\n",
    "file_path = 'path/to/file.csv'\n",
    "\n",
    "# Read in the stories from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    stories = [row['story'] for row in csv_reader]\n",
    "\n",
    "# Define a vectorizer that will convert the stories into a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\n",
    "\n",
    "# Convert the stories into a matrix of TF-IDF features\n",
    "story_matrix = vectorizer.fit_transform(stories)\n",
    "\n",
    "# Loop through the stories and calculate the cosine similarity between each story and the relevant keywords\n",
    "for i, story in enumerate(stories):\n",
    "    # Convert the story into a matrix of TF-IDF features\n",
    "    story_vec = vectorizer.transform([story])\n",
    "\n",
    "    # Calculate the cosine similarity between the story and the relevant keywords\n",
    "    similarity = cosine_similarity(story_vec, story_matrix[:, [vectorizer.vocabulary_.get(word) for word in relevant_keywords]])\n",
    "\n",
    "    # If the similarity is above a certain threshold, print the story\n",
    "   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rake_NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use \"rake_nltk.txt\" to install all the necessary packages** (```pip install -r requirements.txt```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "r = Rake(min_length = 1)\n",
    "\n",
    "r.extract_keywords_from_text(corpus[0])\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyBERT & KeyphraseVectorizers (best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use \"KeyBert_req.txt\" to install all the necessary packages** (```pip install -r requirements.txt```).\n",
    "However, before you install anything, make sure to fulfill the requirements below:\n",
    "* Make sure you installed the following [cuda(especially nvcc)](https://nvidia.github.io/cuda-python/install.html), [spacy](https://spacy.io/usage#quickstart), [visual c++ >2017 and the windows SDK for C++](https://visualstudio.microsoft.com/visual-cpp-build-tools/). The links above should lead you to the installation instruction of each of these libraries in case the pip install of the requirements doesn't work. Visual c++ and the Windows SDK for C++ needs to be installed manually. <u><span style=\"background-color: #f70000\">**Make sure to use Python <= 3.9.**</span><u>\n",
    "    \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from keybert import KeyBERT\n",
    "from paths import DATA \n",
    "\n",
    "#init model\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "corpus = load_data(DATA)[\"File\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer based\n",
    "keywords = kw_model.extract_keywords(\n",
    "        docs=corpus, \n",
    "        vectorizer = KeyphraseCountVectorizer(spacy_pipeline='en_core_web_sm'), #passing vectorizer in, don't use keyphrase_ngram_range\n",
    "        use_maxsum=True, \n",
    "        use_mmr=True,\n",
    "        nr_candidates = 20,\n",
    "        top_n = 10,\n",
    "        diversity=0.5\n",
    "        )\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range\n",
    "keywords_2 = kw_model.extract_keywords(\n",
    "        docs=corpus, \n",
    "        keyphrase_ngram_range = (1,3),\n",
    "        use_maxsum=True, \n",
    "        use_mmr=True,\n",
    "        stop_words ='english', \n",
    "        nr_candidates = 20,\n",
    "        top_n = 10,\n",
    "        diversity=0.5\n",
    "        )\n",
    "\n",
    "keywords_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = [q[0] for x in keywords for q in x]\n",
    "keyphrases_2 = [q[0] for x in keywords_2 for q in x]\n",
    "\n",
    "#change to numpy array \n",
    "import numpy as np\n",
    "\n",
    "keyphrases = np.array(keyphrases)\n",
    "keyphrases_2 = np.array(keyphrases_2)\n",
    "\n",
    "#combine the two arrays into 2-d array\n",
    "keyphrase_arr = np.stack((keyphrases, keyphrases_2), axis = 1)\n",
    "\n",
    "#table of keyphrases\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(keyphrase_arr, columns = ['verctorized_keyphrases', 'ngram_range_keyphrases'])\n",
    "df.to_csv('keyphrases.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "df = pd.read_csv('keyphrases.csv')\n",
    "keyphrases = df['ngram_range_keyphrases'].to_numpy()\n",
    "\n",
    "#embed keyphrases\n",
    "corpus_embeddings = embedder.encode(keyphrases, convert_to_tensor=True)\n",
    "\n",
    "#normalization\n",
    "corpus_embeddings = corpus_embeddings / np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_arr  = []\n",
    "\n",
    "for n_clusters in range(5, 16): \n",
    "    clustering_model = KMeans(n_clusters=n_clusters)\n",
    "    clustering_model.fit(corpus_embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    kmeans_arr.append(np.array(cluster_assignment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_sentences = [[[] for _ in range(n_clusters)] for n_clusters in range(5, 16)]\n",
    "\n",
    "for n_clusters, arr in enumerate(kmeans_arr):\n",
    "    for phrase_id, cluster_id in enumerate(arr):\n",
    "        clustered_sentences[n_clusters][cluster_id].append(keyphrases[phrase_id])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export as json\n",
    "import json\n",
    "index = 10\n",
    "with open(f'clustered_sentences_{index}.json', 'w') as f:\n",
    "\n",
    "    json.dump(clustered_sentences[index], f, indent=4, sort_keys=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "clustering_model_AGC= AgglomerativeClustering(n_clusters=None, distance_threshold=1.5)\n",
    "clustering_model_AGC.fit(corpus_embeddings)\n",
    "cluster_assignment_AGC= clustering_model_AGC.labels_\n",
    "cluster_assignment_AGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_sentences_AGC = {}\n",
    "for phrase_id, cluster_id in enumerate(cluster_assignment_AGC):\n",
    "    if cluster_id not in clustered_sentences:\n",
    "        clustered_sentences[cluster_id] = []\n",
    "\n",
    "    clustered_sentences[cluster_id].append(keyphrases[phrase_id])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cohere umap-learn altair annoy datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate cohere and import libraries\n",
    "import re\n",
    "import umap\n",
    "import json\n",
    "import cohere\n",
    "import pandas as pd \n",
    "import altair as alt\n",
    "from tqdm import tqdm\n",
    "from annoy import AnnoyIndex\n",
    "from keys import COHERE_APIKEY\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "co = cohere.Client(COHERE_APIKEY)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_narratives.csv')\n",
    "narrative_dict = dict(zip(df['id'], df['selftext']))\n",
    "queries = list(json.load(open(\"clustered_sentences_10.json\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0] // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for each query\n",
    "# separate selftex\n",
    "embeds = co.embed(texts=df['selftext'].tolist(), model='embed-english-v2.0' ).embeddings\n",
    "\n",
    "# build index \n",
    "search_index = AnnoyIndex(np.array(embeds).shape[1], 'angular')\n",
    "\n",
    "for index, embed_value in enumerate(embeds):\n",
    "    search_index.add_item(index, embed_value)\n",
    "\n",
    "search_index.build(10)\n",
    "search_index.save(f'search indexes/data_search_index.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_q, query in enumerate(queries): \n",
    "    for index_sub_q, subquery in enumerate(query):\n",
    "        # get embeddings for each query\n",
    "        embeds = co.embed(texts=df['selftext'].tolist(), model='embed-english-v2.0' ).embeddings\n",
    "        \n",
    "        # build index \n",
    "        search_index = AnnoyIndex(np.array(embeds).shape[1], 'angular')\n",
    "\n",
    "        for index, embed_value in enumerate(embeds):\n",
    "            search_index.add_item(index, embed_value)\n",
    "        \n",
    "        search_index.build(10)\n",
    "        search_index.save(f'searches/index_{index_q}/search_index_{index_sub_q}.ann')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g_story = []\n",
    "# Define the keywords and phrases \n",
    "relevant_keywords = ['low socio-economic background', 'higher education', 'difficulties', 'overcome']\n",
    "\n",
    "# Define the keywords and phrases that are likely to appear in irrelevant stories\n",
    "irrelevant_keywords = ['high-achieving', 'wealthy', 'privileged']\n",
    "\n",
    "\n",
    "for story in corpus: \n",
    "    # Check if the story contains any of the relevant keywords\n",
    "    relevant_count = sum(1 for keyword in relevant_keywords if keyword in story.lower())\n",
    "\n",
    "    # Check if the story contains any of the irrelevant keywords\n",
    "    irrelevant_count = sum(1 for keyword in irrelevant_keywords if keyword in story.lower())\n",
    "\n",
    "    # If the story is relevant, print it\n",
    "    if relevant_count > 0 and irrelevant_count == 0:\n",
    "        g_story.append(story)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = corpus\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = [data['text'] for data in labeled_data if data['label'] == 'relevant']\n",
    "train_labels = [data['label'] for data in labeled_data if data['label'] == 'relevant']\n",
    "test_data = [data['text'] for data in labeled_data if data['label'] != 'relevant']\n",
    "test_labels = [data['label'] for data in labeled_data if data['label'] != 'relevant']\n",
    "\n",
    "# Convert the text data into a bag-of-words representation\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_data_counts = vectorizer.fit_transform(train_data)\n",
    "test_data_counts = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier on the labeled data\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_data_counts, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "predictions = clf.predict(test_data_counts)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Filter the relevant stories from the corpus\n",
    "with open('corpus.json', 'r') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "relevant_stories = []\n",
    "for story in corpus:\n",
    "    story_counts = vectorizer.transform([story['text']])\n",
    "    if clf.predict(story_counts) == 'relevant':\n",
    "        relevant_stories.append(story)\n",
    "\n",
    "print(f\"Found {len(relevant_stories)} relevant stories.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
