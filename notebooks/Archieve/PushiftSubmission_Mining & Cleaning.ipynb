{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7aa6ca66",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd524a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "#from pmaw import PushshiftAPI\n",
    "from pathlib import Path \n",
    "# from datetime import datetime, timezone, timedelta  ## incase timezone modifcations are needed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "# import dask.dataframe as dd\n",
    "import math\n",
    "\n",
    "PAR_LENGTH = 150 # in number of words\n",
    "FILTER_REQ = 0.7 # evaluation value for number of paragraphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6d4d17d",
   "metadata": {},
   "source": [
    "### API and Search Keyword Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ca846",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r\"C:\\Users\\MoRevolution\\Desktop\\College\\P&R\\Research_LowSES\\Data Collection\\Reddit Search Keywords/*.txt\", \n",
    "                   recursive = True)\n",
    "\n",
    "filenames = []\n",
    "for i in range(len(files)): \n",
    "    filenames.append(files[i][files[i].rfind(\"\\\\\")+1:-4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4014dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7326f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = []\n",
    "master_keyword_list = []\n",
    "filename_counter = 0\n",
    "\n",
    "for f in files: \n",
    "    with open(f, 'r') as file:\n",
    "        keyword_list = []\n",
    "        for line in file:\n",
    "            keyword_list.append(line.strip('\\n')) \n",
    "            \n",
    "    keyword_list = [i for i in keyword_list if i]\n",
    "    master_keyword_list.append(keyword_list)\n",
    "\n",
    "# print(master_keyword_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b24e410f",
   "metadata": {},
   "source": [
    "### Version 1 (without self text filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c734cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = [\"college\", \"AskReddit\", \"science\", \"psychology\", \"socialwork\", \"personalfinance\", \"financialaid\"]\n",
    "limit = 1000\n",
    "num_workers = 16*2\n",
    "columnNames = ['author', 'selftext', 'url', 'permalink', 'subreddit', 'title', 'keyword', 'keyword_category', 'Number_of_Paragraphs']\n",
    "keyword_id = [\"MAN\", \"MDF\", \"SVO\"]\n",
    "\n",
    "count = 0\n",
    "\n",
    "df_final = pd.DataFrame()\n",
    "for keywords in master_keyword_list: \n",
    "    for subreddit in subreddit_list: \n",
    "        for keyword in keywords: \n",
    "            subs = api.search_submissions(\n",
    "                subreddit= subreddit,\n",
    "                q= keyword,\n",
    "                filter = ['author', 'selftext', 'url', 'permalink', 'subreddit', 'title'],\n",
    "                limit = limit, \n",
    "                num_workers = num_workers\n",
    "                # metadata = \"false\",   \n",
    "                # max_results_per_request= 500\n",
    "            )\n",
    "            \n",
    "            df = pd.DataFrame([rows.d_ for rows in subs])\n",
    "            # df[\"keyword_category\"] = keyword_id[count]\n",
    "            df[\"keyword\"] = keyword\n",
    "            try: \n",
    "                df[\"Number_of_Paragraphs\"] = df['selftext'].apply(lambda n: len(n.split())/PAR_LENGTH)\n",
    "            except: \n",
    "                continue\n",
    "\n",
    "            df = df[df[\"Number_of_Paragraphs\"] >= FILTER_REQ]\n",
    "            df_final = pd.concat([df_final, df])\n",
    "    df_final.to_csv(\"Data Set\"/keyword_id[count]+\".csv\", index= False)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54823744",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns\n",
    "df_final.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67419043",
   "metadata": {},
   "source": [
    "Working Trial, only to categorize data frames into separate directories based on keywords used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggestions\n",
    "    ## write a script that tries out multiple word combinations for all of the keywords already provided \n",
    "    ## consider using cudf instead of pandas for faster dataframe processing\n",
    "    ## test this code using dask for CPU parallel computing\n",
    "subreddit_list = [\"college\", \"AskReddit\", \"science\", \"psychology\", \"socialwork\", \"personalfinance\", \"financialaid\"]\n",
    "limit = 1000\n",
    "num_workers = 16*2\n",
    "\n",
    "for keyword_n in master_keyword_list:\n",
    "    count = 0 \n",
    "    for keywords in keyword_list: \n",
    "        for subreddit_n in subreddit_list:   \n",
    "            subs = api.search_submissions(\n",
    "                subreddit= subreddit_n,\n",
    "                q= keywords,\n",
    "                filter = [ 'author', 'selftext', 'url', 'permalink', 'subreddit', 'title'],\n",
    "                # metadata = \"false\",   \n",
    "                # max_results_per_request= 500\n",
    "                limit = limit, \n",
    "                num_workers = num_workers\n",
    "                )\n",
    "            \n",
    "            # Set display\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "            df = pd.DataFrame([row for row in subs])\n",
    "            ddf = dd.from_pandas(df, npartitions=6)\n",
    "\n",
    "            try: \n",
    "                ddf[\"Number of Paragraphs\"] = ddf['selftext'].apply(lambda n: len(n.split())/PAR_LENGTH)\n",
    "            except:\n",
    "\n",
    "                continue\n",
    "                \n",
    "            df_filtered = ddf[ddf[\"Number of Paragraphs\"] >= FILTER_REQ]\n",
    "\n",
    "            filepath = Path('C:/Users/MoRevolution/Desktop/College/Data Dump/Reddit_1/Submissions/'+ subreddit_n +'/'+filenames[filename_counter]+'/Keyword_'+ str(count)+'.csv')\n",
    "            filepath.parent.mkdir(parents = True, exist_ok = True)\n",
    "            df_filtered.to_csv(filepath,index=False, single_file=True)\n",
    "            count += 1 \n",
    "    filename_counter += 1 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18083c0c",
   "metadata": {},
   "source": [
    "### Version 2 (selftext filter enable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = [\"college\", \"AskReddit\", \"science\", \"psychology\", \"socialwork\", \"personalfinance\", \"financialaid\"]\n",
    "limit = 1000\n",
    "num_workers = 16*2\n",
    "columnNames = ['author', 'selftext', 'url', 'permalink', 'subreddit', 'title', 'keyword', 'keyword_category', 'Number_of_Paragraphs']\n",
    "keyword_id = [\"MAN\", \"MDF\", \"SVO\"]\n",
    "\n",
    "count = 0\n",
    "\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "for keywords in master_keyword_list: \n",
    "    for subreddit in subreddit_list: \n",
    "        for keyword in keywords: \n",
    "            subs = api.search_submissions(\n",
    "                subreddit= subreddit,\n",
    "                q= keyword,\n",
    "                filter = ['author', 'selftext', 'url', 'permalink', 'subreddit', 'title'],\n",
    "                # selftext = add  or to make limit categories and use OR for more subcategories\n",
    "                limit = limit, \n",
    "                num_workers = num_workers\n",
    "                # metadata = \"false\",   \n",
    "                # max_results_per_request= 500\n",
    "            )\n",
    "            \n",
    "            df = pd.DataFrame([rows.d_ for rows in subs])\n",
    "            # df[\"keyword_category\"] = keyword_id[count]\n",
    "            df[\"keyword\"] = keyword\n",
    "            \n",
    "            try: \n",
    "                df[\"Number_of_Paragraphs\"] = df['selftext'].apply(lambda n: len(n.split())/PAR_LENGTH)\n",
    "            except: \n",
    "                continue\n",
    "\n",
    "            df = df[df[\"Number_of_Paragraphs\"] >= FILTER_REQ]\n",
    "            df_final = pd.concat([df_final, df])\n",
    "    df_final.to_csv(\"Data Set\"/keyword_id[count]+\".csv\", index= False)\n",
    "    count += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0234241",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "- Fix problem with apostrophes being replaced by three different symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ff0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv('Data Set/Man.csv')\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['selftext'] = ddf['selftext'].replace('â€™','\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8cbfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.to_csv(\"Data Set/Filtered and SQL/Man.csv\",index = False, single_file = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
