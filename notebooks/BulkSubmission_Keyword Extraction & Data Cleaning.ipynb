{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f) \n",
    "    return (data)\n",
    "\n",
    "corpus = load_data(\"local_data.json\")[\"File\"]\n",
    "# corpus[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Word extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to extract keywords\n",
    "def extract_keywords(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "    # Tag the tokens with their part of speech\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    # Define a function to convert part of speech tags to WordNet compatible tags\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    # Lemmatize the tokens using WordNet\n",
    "    lemmatizer = wordnet.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    # Identify named entities using NLTK's named entity recognition (NER) module\n",
    "    named_entities = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "\n",
    "    # Extract the most common lemmas and named entities\n",
    "    keywords = [token for token, count in Counter(lemmatized_tokens + [chunk[0] for chunk in named_entities if hasattr(chunk, 'label')]).most_common(10)]\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download using nltk.download('punkt') if you get an nltk error\n",
    "all_keywords = []\n",
    "for index, story in enumerate(corpus):\n",
    "    keywords = extract_keywords(story[index])\n",
    "    all_keywords.append(keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the stop words for English\n",
    "spacy_stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# choose the set of english stopwords\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess the input corpus\n",
    "def preprocess_text(text):\n",
    "    # tokenize a story within a corpus \n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # remove alphanumeric characters and stop words\n",
    "    cleaned_tokens = [token for token in tokens if token.isalpha() and token not in spacy_stop_words]\n",
    "    return cleaned_tokens\n",
    "\n",
    "cleaned_corpus = [preprocess_text(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorize using TfidfVectorizer which combines counting and noralized weighting\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_keywords(tfidf_scores, feature_names, top_n):\n",
    "    sorted_scores = np.argsort(tfidf_scores)[::-1]\n",
    "    top_keywords = [feature_names[i] for i in sorted_scores[:top_n]]\n",
    "    return top_keywords\n",
    "\n",
    "for i, doc in enumerate(corpus):\n",
    "    tfidf_scores = tfidf_matrix[i].toarray()[0]\n",
    "    top_keywords = get_top_keywords(tfidf_scores, feature_names, 10)\n",
    "    print(f\"Document {i+1} top keywords: {top_keywords}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimental "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the keywords and phrases that are likely to appear in relevant stories\n",
    "relevant_keywords = ['low socio-economic background', 'higher education', 'difficulties', 'overcome']\n",
    "\n",
    "# Define the path to the file containing the stories\n",
    "file_path = 'path/to/file.csv'\n",
    "\n",
    "# Read in the stories from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    stories = [row['story'] for row in csv_reader]\n",
    "\n",
    "# Define a vectorizer that will convert the stories into a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\n",
    "\n",
    "# Convert the stories into a matrix of TF-IDF features\n",
    "story_matrix = vectorizer.fit_transform(stories)\n",
    "\n",
    "# Loop through the stories and calculate the cosine similarity between each story and the relevant keywords\n",
    "for i, story in enumerate(stories):\n",
    "    # Convert the story into a matrix of TF-IDF features\n",
    "    story_vec = vectorizer.transform([story])\n",
    "\n",
    "    # Calculate the cosine similarity between the story and the relevant keywords\n",
    "    similarity = cosine_similarity(story_vec, story_matrix[:, [vectorizer.vocabulary_.get(word) for word in relevant_keywords]])\n",
    "\n",
    "    # If the similarity is above a certain threshold, print the story\n",
    "   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rake_NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use \"rake_nltk.txt\" to install all the necessary packages** (```pip install -r requirements.txt```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "r = Rake(min_length = 1)\n",
    "\n",
    "r.extract_keywords_from_text(corpus[0])\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KeyBERT & KeyphraseVectorizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use \"KeyBert_req.txt\" to install all the necessary packages** (```pip install -r requirements.txt```).\n",
    "However, before you install anything, make sure to fulfill the requirements below:\n",
    "* Make sure you installed the following [cuda(especially nvcc)](https://nvidia.github.io/cuda-python/install.html), [spacy](https://spacy.io/usage#quickstart), [visual c++ >2017 and the windows SDK for C++](https://visualstudio.microsoft.com/visual-cpp-build-tools/). The links above should lead you to the installation instruction of each of these libraries in case the pip install of the requirements doesn't work. Visual c++ and the Windows SDK for C++ needs to be installed manually. <u><span style=\"background-color: #f70000\">**Make sure to use Python <= 3.9.**</span><u>\n",
    "* <u>**I highly highly suggest the use of a virtual environment when installing all the python libraries and dependencies.**<u>\n",
    "    \n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from keybert import KeyBERT\n",
    "\n",
    "#init model\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using an ngram_range\n",
    "# keywords_1 = kw_model.extract_keywords(\n",
    "#         docs=corpus, \n",
    "#         # vectorizer = KeyphraseCountVectorizer(), #passing vectorizer in, don't use keyphrase_ngram_range\n",
    "#         keyphrase_ngram_range= (1,3),\n",
    "#         stop_words ='english', \n",
    "#         use_maxsum=True, \n",
    "#         use_mmr=True,\n",
    "#         nr_candidates = 20,\n",
    "#         top_n = 20, \n",
    "#         diversity=0.5\n",
    "#         )\n",
    "\n",
    "keywords_2 = kw_model.extract_keywords(\n",
    "        docs=corpus, \n",
    "        vectorizer = KeyphraseCountVectorizer(spacy_pipeline='en_core_web_sm', workers=5), #passing vectorizer in, don't use keyphrase_ngram_range\n",
    "        use_maxsum=True, \n",
    "        use_mmr=True,\n",
    "        nr_candidates = 20,\n",
    "        top_n = 10,\n",
    "        diversity=0.5\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('community college', 0.3562),\n",
       " ('affirmative action type candidate', 0.356),\n",
       " ('government programs', 0.2948),\n",
       " ('more job counseling', 0.2814),\n",
       " ('poverty', 0.2588),\n",
       " ('accounting', 0.1879),\n",
       " ('more it classes', 0.1594),\n",
       " ('house insecure', 0.1322),\n",
       " ('midwest', 0.1315),\n",
       " ('way', 0.0915)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_2[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My mum had me at 15 years. No idea who my dad is. I grew up with a single mum who would spend every last dollar on meth or coke. To say we were poor was an understatement. No amount of government assistance can get through to you if your mother is an addict. We moved around a lot, I went to 17 different schools growing up, having no food was a common occurrence. I've been homeless for periods of time as a kid. I've had to wash myself in public restrooms and from time to time I was sent to other 'relatives' to live. I was sexually abused on multiple occasions, and I've kept all of this to myself all these years. When you're a kid it's terrifying to speak out. You already live in a shaky, unstable world so uprooting the last foundation you have, even if it's a drug addled mother is unthinkable. Anyway, fast-forward. I tried really hard in school. I mean really hard. It was the only way I could see myself getting out of the hole I was in. My mum dropped out of school at 14 and all I knew is that I never wanted to end up like her. I got a job the day of my 15th birthday which in my country is the legal age you can start working and to this day, I'm 27 years now, I've not spent a day unemployed. I worked and saved as much as I could and when mum told me she was moving again when I was 16, I said no and moved out on my own. I was tired of starting over. I applied for emancipation and moved into a flat. I've seen her a handful of times since, I'm not even sure where she lives anymore.I went to university, kept up a 4.0 GPA while working near full-time and graduated with first class honors. I don't say this to brag. I sacrificed a lot to pull this off. I trashed my social life, never went on a holiday and ignored parts of my health because I wasn't a prodigy or anything close to it, I just fucking grinded my face off. I got a job in my field, got a post degree qualification, and in the last couple of years I've started clearing $100k+ per year.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "# import zstandard as zstd\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\"\n",
    "# temp_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\temp\\RS_2005-06.zst\"\n",
    "decompressed_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\\Decompressed\\RS_2018-06.json\"\n",
    "output_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\\Decompressed\\chunks\"\n",
    "# listing = glob.glob(temp_path + '\\\\*.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=50000\n",
    "batch_no=1 \n",
    "\n",
    "for chunk in pd.read_json(decompressed_path,lines=True,chunksize=chunk_size):\n",
    "    chunk.to_csv(os.path.join(output_path, str(batch_no)+'.csv'),index=False)\n",
    "    batch_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "# import zstandard as zstd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# compressed_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\"\n",
    "# temp_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\temp\\RS_2005-06.zst\"\n",
    "decompressed_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\\Decompressed\\RS_2018-06.json\"\n",
    "output_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\\Decompressed\\chunks\"\n",
    "# listing = glob.glob(temp_path + '\\\\*.zst')\n",
    "chunk_size=50000\n",
    "batch_no=1 \n",
    "\n",
    "for chunk in pd.read_json(decompressed_path,lines=True,chunksize=chunk_size):\n",
    "    chunk.to_csv(os.path.join(output_path, str(batch_no)+'.csv'),index=False)\n",
    "    batch_no+=1\n",
    "path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\\Decompressed\\chunks\\1.csv\"\n",
    "df = pd.read_csv(path)\n",
    "arr = np.array([\"college\", \"AskReddit\", \"science\", \"psychology\", \"socialwork\", \"personalfinance\", \"financialaid\"])\n",
    "count = (df['subreddit'].isin(arr)).sum()\n",
    "\n",
    "counts = df['subreddit'].value_counts()[arr]\n",
    "print(counts)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\\Decompressed\\chunks\\1.csv\"\n",
    "df = pd.read_csv(path)\n",
    "arr = np.array([\"college\", \"AskReddit\", \"science\", \"psychology\", \"socialwork\", \"personalfinance\", \"financialaid\"])\n",
    "count = (df['subreddit'].isin(arr)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['subreddit'].value_counts()[arr]\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g_story = []\n",
    "# Define the keywords and phrases \n",
    "relevant_keywords = ['low socio-economic background', 'higher education', 'difficulties', 'overcome']\n",
    "\n",
    "# Define the keywords and phrases that are likely to appear in irrelevant stories\n",
    "irrelevant_keywords = ['high-achieving', 'wealthy', 'privileged']\n",
    "\n",
    "\n",
    "for story in corpus: \n",
    "    # Check if the story contains any of the relevant keywords\n",
    "    relevant_count = sum(1 for keyword in relevant_keywords if keyword in story.lower())\n",
    "\n",
    "    # Check if the story contains any of the irrelevant keywords\n",
    "    irrelevant_count = sum(1 for keyword in irrelevant_keywords if keyword in story.lower())\n",
    "\n",
    "    # If the story is relevant, print it\n",
    "    if relevant_count > 0 and irrelevant_count == 0:\n",
    "        g_story.append(story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = corpus\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = [data['text'] for data in labeled_data if data['label'] == 'relevant']\n",
    "train_labels = [data['label'] for data in labeled_data if data['label'] == 'relevant']\n",
    "test_data = [data['text'] for data in labeled_data if data['label'] != 'relevant']\n",
    "test_labels = [data['label'] for data in labeled_data if data['label'] != 'relevant']\n",
    "\n",
    "# Convert the text data into a bag-of-words representation\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_data_counts = vectorizer.fit_transform(train_data)\n",
    "test_data_counts = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier on the labeled data\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_data_counts, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "predictions = clf.predict(test_data_counts)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Filter the relevant stories from the corpus\n",
    "with open('corpus.json', 'r') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "relevant_stories = []\n",
    "for story in corpus:\n",
    "    story_counts = vectorizer.transform([story['text']])\n",
    "    if clf.predict(story_counts) == 'relevant':\n",
    "        relevant_stories.append(story)\n",
    "\n",
    "print(f\"Found {len(relevant_stories)} relevant stories.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
