{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa6ca66",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd524a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "#from pmaw import PushshiftAPI\n",
    "from pathlib import Path \n",
    "# from datetime import datetime, timezone, timedelta  ## incase timezone modifcations are needed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "# import dask.dataframe as dd\n",
    "import math\n",
    "\n",
    "PAR_LENGTH = 150 # in number of words\n",
    "FILTER_REQ = 0.7 # evaluation value for number of paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4d17d",
   "metadata": {},
   "source": [
    "### API and Search Keyword Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063ca846",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r\"C:\\Users\\MoRevolution\\Desktop\\College\\P&R\\Research_LowSES\\Data Collection\\Reddit Search Keywords/*.txt\", \n",
    "                   recursive = True)\n",
    "\n",
    "filenames = []\n",
    "for i in range(len(files)): \n",
    "    filenames.append(files[i][files[i].rfind(\"\\\\\")+1:-4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4014dd8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MoRevolution\\Desktop\\College\\P&R\\Research_LowSES\\Data Collection\\Submissions_Build&Clean.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MoRevolution/Desktop/College/P%26R/Research_LowSES/Data%20Collection/Submissions_Build%26Clean.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m api \u001b[39m=\u001b[39m PushshiftAPI()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\psaw\\PushshiftAPI.py:326\u001b[0m, in \u001b[0;36mPushshiftAPI.__init__\u001b[1;34m(self, r, *args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, r\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    290\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[39m    Helper class for interacting with the PushShift API for searching public reddit archival data.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m    :type shards_down_behavior: str, optional\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mr \u001b[39m=\u001b[39m r\n\u001b[0;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_search_func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_search\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\psaw\\PushshiftAPI.py:94\u001b[0m, in \u001b[0;36mPushshiftAPIMinimal.__init__\u001b[1;34m(self, max_retries, max_sleep, backoff, rate_limit_per_minute, max_results_per_request, detect_local_tz, utc_offset_secs, domain, https_proxy, shards_down_behavior)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m rate_limit_per_minute \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mConnecting to /meta endpoint to learn rate limit.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_url\u001b[39m.\u001b[39;49mformat(endpoint\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmeta\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     95\u001b[0m     rate_limit_per_minute \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39mserver_ratelimit_per_minute\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     96\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mserver_ratelimit_per_minute: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m rate_limit_per_minute)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\psaw\\PushshiftAPI.py:181\u001b[0m, in \u001b[0;36mPushshiftAPIMinimal._get\u001b[1;34m(self, url, payload)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    180\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mUnable to connect to pushshift.io. Retrying after backoff.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 181\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_impose_rate_limit(i)\n\u001b[0;32m    182\u001b[0m i\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    183\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\psaw\\PushshiftAPI.py:151\u001b[0m, in \u001b[0;36mPushshiftAPIMinimal._impose_rate_limit\u001b[1;34m(self, nth_request)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39mif\u001b[39;00m interval \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    150\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mImposing rate limit, sleeping for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m interval)\n\u001b[1;32m--> 151\u001b[0m     time\u001b[39m.\u001b[39;49msleep(interval)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7326f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 23: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MoRevolution\\Desktop\\College\\P&R\\Research_LowSES\\Data Collection\\Submissions_Build&Clean.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MoRevolution/Desktop/College/P%26R/Research_LowSES/Data%20Collection/Submissions_Build%26Clean.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(f, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MoRevolution/Desktop/College/P%26R/Research_LowSES/Data%20Collection/Submissions_Build%26Clean.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     keyword_list \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MoRevolution/Desktop/College/P%26R/Research_LowSES/Data%20Collection/Submissions_Build%26Clean.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MoRevolution/Desktop/College/P%26R/Research_LowSES/Data%20Collection/Submissions_Build%26Clean.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         keyword_list\u001b[39m.\u001b[39mappend(line\u001b[39m.\u001b[39mstrip(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MoRevolution/Desktop/College/P%26R/Research_LowSES/Data%20Collection/Submissions_Build%26Clean.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m keyword_list \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m keyword_list \u001b[39mif\u001b[39;00m i]\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 23: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "keyword_list = []\n",
    "master_keyword_list = []\n",
    "filename_counter = 0\n",
    "\n",
    "for f in files: \n",
    "    with open(f, 'r') as file:\n",
    "        keyword_list = []\n",
    "        for line in file:\n",
    "            keyword_list.append(line.strip('\\n')) \n",
    "            \n",
    "    keyword_list = [i for i in keyword_list if i]\n",
    "    master_keyword_list.append(keyword_list)\n",
    "\n",
    "# print(master_keyword_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e410f",
   "metadata": {},
   "source": [
    "### Version 1 (without self text filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c734cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = [\"college\", \"AskReddit\", \"science\", \"psychology\", \"socialwork\", \"personalfinance\", \"financialaid\"]\n",
    "limit = 1000\n",
    "num_workers = 16*2\n",
    "columnNames = ['author', 'selftext', 'url', 'permalink', 'subreddit', 'title', 'keyword', 'keyword_category', 'Number_of_Paragraphs']\n",
    "keyword_id = [\"MAN\", \"MDF\", \"SVO\"]\n",
    "\n",
    "count = 0\n",
    "\n",
    "df_final = pd.DataFrame()\n",
    "for keywords in master_keyword_list: \n",
    "    for subreddit in subreddit_list: \n",
    "        for keyword in keywords: \n",
    "            subs = api.search_submissions(\n",
    "                subreddit= subreddit,\n",
    "                q= keyword,\n",
    "                filter = ['author', 'selftext', 'url', 'permalink', 'subreddit', 'title'],\n",
    "                limit = limit, \n",
    "                num_workers = num_workers\n",
    "                # metadata = \"false\",   \n",
    "                # max_results_per_request= 500\n",
    "            )\n",
    "            \n",
    "            df = pd.DataFrame([rows.d_ for rows in subs])\n",
    "            # df[\"keyword_category\"] = keyword_id[count]\n",
    "            df[\"keyword\"] = keyword\n",
    "            try: \n",
    "                df[\"Number_of_Paragraphs\"] = df['selftext'].apply(lambda n: len(n.split())/PAR_LENGTH)\n",
    "            except: \n",
    "                continue\n",
    "\n",
    "            df = df[df[\"Number_of_Paragraphs\"] >= FILTER_REQ]\n",
    "            df_final = pd.concat([df_final, df])\n",
    "    df_final.to_csv(\"Data Set\"/keyword_id[count]+\".csv\", index= False)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54823744",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns\n",
    "df_final.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67419043",
   "metadata": {},
   "source": [
    "Working Trial, only to categorize data frames into separate directories based on keywords used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80b1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggestions\n",
    "    ## write a script that tries out multiple word combinations for all of the keywords already provided \n",
    "    ## consider using cudf instead of pandas for faster dataframe processing\n",
    "    ## test this code using dask for CPU parallel computing\n",
    "subreddit_list = [\"college\", \"AskReddit\", \"science\", \"psychology\", \"socialwork\", \"personalfinance\", \"financialaid\"]\n",
    "limit = 1000\n",
    "num_workers = 16*2\n",
    "\n",
    "for keyword_n in master_keyword_list:\n",
    "    count = 0 \n",
    "    for keywords in keyword_list: \n",
    "        for subreddit_n in subreddit_list:   \n",
    "            subs = api.search_submissions(\n",
    "                subreddit= subreddit_n,\n",
    "                q= keywords,\n",
    "                filter = [ 'author', 'selftext', 'url', 'permalink', 'subreddit', 'title'],\n",
    "                # metadata = \"false\",   \n",
    "                # max_results_per_request= 500\n",
    "                limit = limit, \n",
    "                num_workers = num_workers\n",
    "                )\n",
    "            \n",
    "            # Set display\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "            df = pd.DataFrame([row for row in subs])\n",
    "            ddf = dd.from_pandas(df, npartitions=6)\n",
    "\n",
    "            try: \n",
    "                ddf[\"Number of Paragraphs\"] = ddf['selftext'].apply(lambda n: len(n.split())/PAR_LENGTH)\n",
    "            except:\n",
    "\n",
    "                continue\n",
    "                \n",
    "            df_filtered = ddf[ddf[\"Number of Paragraphs\"] >= FILTER_REQ]\n",
    "\n",
    "            filepath = Path('C:/Users/MoRevolution/Desktop/College/Data Dump/Reddit_1/Submissions/'+ subreddit_n +'/'+filenames[filename_counter]+'/Keyword_'+ str(count)+'.csv')\n",
    "            filepath.parent.mkdir(parents = True, exist_ok = True)\n",
    "            df_filtered.to_csv(filepath,index=False, single_file=True)\n",
    "            count += 1 \n",
    "    filename_counter += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18083c0c",
   "metadata": {},
   "source": [
    "### Version 2 (selftext filter enable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_list = [\"college\", \"AskReddit\", \"science\", \"psychology\", \"socialwork\", \"personalfinance\", \"financialaid\"]\n",
    "limit = 1000\n",
    "num_workers = 16*2\n",
    "columnNames = ['author', 'selftext', 'url', 'permalink', 'subreddit', 'title', 'keyword', 'keyword_category', 'Number_of_Paragraphs']\n",
    "keyword_id = [\"MAN\", \"MDF\", \"SVO\"]\n",
    "\n",
    "count = 0\n",
    "\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "for keywords in master_keyword_list: \n",
    "    for subreddit in subreddit_list: \n",
    "        for keyword in keywords: \n",
    "            subs = api.search_submissions(\n",
    "                subreddit= subreddit,\n",
    "                q= keyword,\n",
    "                filter = ['author', 'selftext', 'url', 'permalink', 'subreddit', 'title'],\n",
    "                # selftext = add  or to make limit categories and use OR for more subcategories\n",
    "                limit = limit, \n",
    "                num_workers = num_workers\n",
    "                # metadata = \"false\",   \n",
    "                # max_results_per_request= 500\n",
    "            )\n",
    "            \n",
    "            df = pd.DataFrame([rows.d_ for rows in subs])\n",
    "            # df[\"keyword_category\"] = keyword_id[count]\n",
    "            df[\"keyword\"] = keyword\n",
    "            \n",
    "            try: \n",
    "                df[\"Number_of_Paragraphs\"] = df['selftext'].apply(lambda n: len(n.split())/PAR_LENGTH)\n",
    "            except: \n",
    "                continue\n",
    "\n",
    "            df = df[df[\"Number_of_Paragraphs\"] >= FILTER_REQ]\n",
    "            df_final = pd.concat([df_final, df])\n",
    "    df_final.to_csv(\"Data Set\"/keyword_id[count]+\".csv\", index= False)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0234241",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "- Fix problem with apostrophes being replaced by three different symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b01ff0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>permalink</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>created</th>\n",
       "      <th>keyword</th>\n",
       "      <th>Number_of_Paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Real_Librarian_3294</td>\n",
       "      <td>1669165200</td>\n",
       "      <td>/r/college/comments/z2ave9/will_i_have_to_drop...</td>\n",
       "      <td>Things have been rough on my college journey. ...</td>\n",
       "      <td>college</td>\n",
       "      <td>Will I have to dropout?</td>\n",
       "      <td>https://www.reddit.com/r/college/comments/z2av...</td>\n",
       "      <td>1.669187e+09</td>\n",
       "      <td>can't pay for school</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spodosolluvr</td>\n",
       "      <td>1669137743</td>\n",
       "      <td>/r/college/comments/z1z9nc/renting_in_college_...</td>\n",
       "      <td>Hi everyone!! I'm a junior going to a large re...</td>\n",
       "      <td>college</td>\n",
       "      <td>renting in college help</td>\n",
       "      <td>https://www.reddit.com/r/college/comments/z1z9...</td>\n",
       "      <td>1.669159e+09</td>\n",
       "      <td>can't pay for school</td>\n",
       "      <td>4.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thecoolan</td>\n",
       "      <td>1668323192</td>\n",
       "      <td>/r/college/comments/ytugtj/i_think_im_gonna_dr...</td>\n",
       "      <td>&amp;amp;#x200B;\\n\\n***I don't think anybody will ...</td>\n",
       "      <td>college</td>\n",
       "      <td>I think I'm gonna drop out; I regret ever goin...</td>\n",
       "      <td>https://www.reddit.com/r/college/comments/ytug...</td>\n",
       "      <td>1.668345e+09</td>\n",
       "      <td>can't pay for school</td>\n",
       "      <td>2.386667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BATZ202</td>\n",
       "      <td>1668179293</td>\n",
       "      <td>/r/college/comments/ysd45e/i_feel_so_tired_of_...</td>\n",
       "      <td>I'm going through college by myself, my parent...</td>\n",
       "      <td>college</td>\n",
       "      <td>I feel so tired of everything</td>\n",
       "      <td>https://www.reddit.com/r/college/comments/ysd4...</td>\n",
       "      <td>1.668201e+09</td>\n",
       "      <td>can't pay for school</td>\n",
       "      <td>1.993333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qweezek</td>\n",
       "      <td>1668022332</td>\n",
       "      <td>/r/college/comments/yqt06e/i_am_depressed_beca...</td>\n",
       "      <td>So, first thing I want to say is that I am not...</td>\n",
       "      <td>college</td>\n",
       "      <td>I am depressed because my parents pay for my s...</td>\n",
       "      <td>https://www.reddit.com/r/college/comments/yqt0...</td>\n",
       "      <td>1.668044e+09</td>\n",
       "      <td>can't pay for school</td>\n",
       "      <td>4.793333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author  created_utc  \\\n",
       "0  Real_Librarian_3294   1669165200   \n",
       "1         spodosolluvr   1669137743   \n",
       "2            thecoolan   1668323192   \n",
       "3              BATZ202   1668179293   \n",
       "4              qweezek   1668022332   \n",
       "\n",
       "                                           permalink  \\\n",
       "0  /r/college/comments/z2ave9/will_i_have_to_drop...   \n",
       "1  /r/college/comments/z1z9nc/renting_in_college_...   \n",
       "2  /r/college/comments/ytugtj/i_think_im_gonna_dr...   \n",
       "3  /r/college/comments/ysd45e/i_feel_so_tired_of_...   \n",
       "4  /r/college/comments/yqt06e/i_am_depressed_beca...   \n",
       "\n",
       "                                            selftext subreddit  \\\n",
       "0  Things have been rough on my college journey. ...   college   \n",
       "1  Hi everyone!! I'm a junior going to a large re...   college   \n",
       "2  &amp;#x200B;\\n\\n***I don't think anybody will ...   college   \n",
       "3  I'm going through college by myself, my parent...   college   \n",
       "4  So, first thing I want to say is that I am not...   college   \n",
       "\n",
       "                                               title  \\\n",
       "0                            Will I have to dropout?   \n",
       "1                            renting in college help   \n",
       "2  I think I'm gonna drop out; I regret ever goin...   \n",
       "3                      I feel so tired of everything   \n",
       "4  I am depressed because my parents pay for my s...   \n",
       "\n",
       "                                                 url       created  \\\n",
       "0  https://www.reddit.com/r/college/comments/z2av...  1.669187e+09   \n",
       "1  https://www.reddit.com/r/college/comments/z1z9...  1.669159e+09   \n",
       "2  https://www.reddit.com/r/college/comments/ytug...  1.668345e+09   \n",
       "3  https://www.reddit.com/r/college/comments/ysd4...  1.668201e+09   \n",
       "4  https://www.reddit.com/r/college/comments/yqt0...  1.668044e+09   \n",
       "\n",
       "                keyword  Number_of_Paragraphs  \n",
       "0  can't pay for school              0.840000  \n",
       "1  can't pay for school              4.433333  \n",
       "2  can't pay for school              2.386667  \n",
       "3  can't pay for school              1.993333  \n",
       "4  can't pay for school              4.793333  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = dd.read_csv('Data Set/Man.csv')\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "016f92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['selftext'] = ddf['selftext'].replace('â€™','\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad8cbfab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\MoRevolution\\\\Desktop\\\\College\\\\P&R\\\\Research_LowSES\\\\Data Collection\\\\Data Set\\\\Filtered and SQL\\\\Man.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.to_csv(\"Data Set/Filtered and SQL/Man.csv\",index = False, single_file = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
