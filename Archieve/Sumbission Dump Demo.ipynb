{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5fd524a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MoRevolution\\Desktop\\College\\P&R\\Research_LowSES\\Data Collection\\Reddit Search Keywords\\Manual Search Keywords.txt\n",
      "C:\\Users\\MoRevolution\\Desktop\\College\\P&R\\Research_LowSES\\Data Collection\\Reddit Search Keywords\\Modified Keywords .txt\n",
      "C:\\Users\\MoRevolution\\Desktop\\College\\P&R\\Research_LowSES\\Data Collection\\Reddit Search Keywords\\SVO Search Keywords.txt\n",
      "Manual Search Keywords\n",
      "Modified Keywords \n",
      "SVO Search Keywords\n"
     ]
    }
   ],
   "source": [
    "from psaw import PushshiftAPI\n",
    "from pathlib import Path \n",
    "# from datetime import datetime, timezone, timedelta  ## incase timezone modifcations are needed\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob \n",
    "\n",
    "files = glob.glob(r\"C:\\Users\\MoRevolution\\Desktop\\College\\P&R\\Research_LowSES\\Data Collection\\Reddit Search Keywords/*.txt\", \n",
    "                   recursive = True)\n",
    "\n",
    "for f in files: \n",
    "    print(f)\n",
    "\n",
    "\n",
    "filenames = []\n",
    "for i in range(len(files)): \n",
    "    filenames.append(files[i][files[i].rfind(\"\\\\\")+1:-4])\n",
    "\n",
    "for f in filenames: \n",
    "    print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01172339",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d7326f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"can't pay for school\", 'struggling with tuition', 'rejected because of finances when applying to college', 'imposer syndrome in college ', 'imposter syndrome in a competitive highschool', 'school is too expensive', \"can't afford tuition \", 'first-gen college student guilt ', 'first-gen college student family pressure', 'first-gen college student struggling with finances', 'lack of emotional support from parents in college', \"my family can't afford to pay school for me \", \"i can't afford a dorm\"], [\"tried hard in school OR dropped out school OR I was the poorest kid in class OR i can't afford a dorm\", \"i can't afford a dorm OR school is too expensive OR can't pay for school OR first-gen college student struggling with finance\", 'growing in poor family OR I come from low income family of substance abusers OR grew up with single mum OR grew up with single father', 'growing in poor family OR I come from low income family of substance abusers OR first-gen college student family pressure', \"grew up with single mum OR grew up with single father OR my family can't afford to pay school for me\", 'I was the poorest kid in class OR it is hard for me to connect ', 'imposter syndrome in a competitive highschool OR imposer syndrome in college '], ['tried hard in school ', 'dropped out school ', 'grew up with single mum', 'I was the poorest kid in class', 'growing in poor family', 'I come from low income family of substance abusers ', 'feel like I did not belong in school ', 'I grew up relatively poor ', 'it is hard for me to connect ']]\n"
     ]
    }
   ],
   "source": [
    "keyword_list = []\n",
    "master_keyword_list = []\n",
    "filename_counter = 0\n",
    "\n",
    "for f in files: \n",
    "    with open(f, 'r') as file:\n",
    "        keyword_list = []\n",
    "        for line in file:\n",
    "            keyword_list.append(line.strip('\\n')) \n",
    "            \n",
    "    keyword_list = [i for i in keyword_list if i]\n",
    "    master_keyword_list.append(keyword_list)\n",
    "\n",
    "print(master_keyword_list)\n",
    "\n",
    "# Suggestions\n",
    "    ## write a script that tries out multiple word combinations for all of the keywords already provided \n",
    "    ## \n",
    "\n",
    "subreddit_list = [\"college\", \"AskReddit\", \"science\", \"psychology\", \"socialwork\", \"personalfinance\", \"financialaid\"]\n",
    "for keyword_n in master_keyword_list:\n",
    "    count = 0 \n",
    "    for keywords in keyword_list: \n",
    "        for subreddit_n in subreddit_list:   \n",
    "            subs = api.search_submissions(\n",
    "                subreddit= subreddit_n,\n",
    "                q= keywords,\n",
    "                filter = [ 'author', 'selftext', 'url', 'permalink', 'subreddit', 'title'],\n",
    "                # metadata = \"false\",   \n",
    "                # max_results_per_request= 500\n",
    "                )\n",
    "\n",
    "            # Set display\n",
    "            pd.set_option('display.max_rows', None)\n",
    "            pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "            df = pd.DataFrame([thing.d_ for thing in subs])\n",
    "\n",
    "            filepath = Path('Data Dumb/Reddit/Submissions/'+ subreddit_n +'/'+filenames[filename_counter]+'/Keyword_'+ str(count)+'.csv')\n",
    "            filepath.parent.mkdir(parents = True, exist_ok = True)\n",
    "            df.to_csv(filepath)\n",
    "            count += 1 \n",
    "    filename_counter += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80b1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
