{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Relevant Keywords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries and Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MoRevolution\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\MoRevolution\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MoRevolution\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from collections import Counter\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My mum had me at 15 years. No idea who my dad is. I grew up with a single mum who would spend every last dollar on meth or coke. To say we were poor was an understatement. No amount of government assistance can get through to you if your mother is an addict. We moved around a lot, I went to 17 different schools growing up, having no food was a common occurrence. I've been homeless for periods of time as a kid. I've had to wash myself in public restrooms and from time to time I was sent to other 'relatives' to live. I was sexually abused on multiple occasions, and I've kept all of this to myself all these years. When you're a kid it's terrifying to speak out. You already live in a shaky, unstable world so uprooting the last foundation you have, even if it's a drug addled mother is unthinkable. Anyway, fast-forward. I tried really hard in school. I mean really hard. It was the only way I could see myself getting out of the hole I was in. My mum dropped out of school at 14 and all I knew is that I never wanted to end up like her. I got a job the day of my 15th birthday which in my country is the legal age you can start working and to this day, I'm 27 years now, I've not spent a day unemployed. I worked and saved as much as I could and when mum told me she was moving again when I was 16, I said no and moved out on my own. I was tired of starting over. I applied for emancipation and moved into a flat. I've seen her a handful of times since, I'm not even sure where she lives anymore.I went to university, kept up a 4.0 GPA while working near full-time and graduated with first class honors. I don't say this to brag. I sacrificed a lot to pull this off. I trashed my social life, never went on a holiday and ignored parts of my health because I wasn't a prodigy or anything close to it, I just fucking grinded my face off. I got a job in my field, got a post degree qualification, and in the last couple of years I've started clearing $100k+ per year.\n"
     ]
    }
   ],
   "source": [
    "# Load the corpus\n",
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f) \n",
    "    return (data)\n",
    "\n",
    "corpus = load_data(\"local_data.json\")[\"File\"]\n",
    "# print(corpus[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Word extractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to extract keywords\n",
    "def extract_keywords(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwords_list]\n",
    "\n",
    "    # Tag the tokens with their part of speech\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    # Define a function to convert part of speech tags to WordNet compatible tags\n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    # Lemmatize the tokens using WordNet\n",
    "    lemmatizer = wordnet.WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) for token, tag in tagged_tokens]\n",
    "\n",
    "    # Identify named entities using NLTK's named entity recognition (NER) module\n",
    "    named_entities = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "\n",
    "    # Extract the most common lemmas and named entities\n",
    "    keywords = [token for token, count in Counter(lemmatized_tokens + [chunk[0] for chunk in named_entities if hasattr(chunk, 'label')]).most_common(10)]\n",
    "\n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'WordNetCorpusReader' object has no attribute 'WordNetLemmatizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m all_keywords \u001b[39m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m index, story \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(corpus):\n\u001b[1;32m----> 4\u001b[0m     keywords \u001b[39m=\u001b[39m extract_keywords(story[index])\n\u001b[0;32m      5\u001b[0m     all_keywords\u001b[39m.\u001b[39mappend(keywords)\n",
      "Cell \u001b[1;32mIn[26], line 27\u001b[0m, in \u001b[0;36mextract_keywords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[39m# Lemmatize the tokens using WordNet\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m lemmatizer \u001b[39m=\u001b[39m wordnet\u001b[39m.\u001b[39;49mWordNetLemmatizer()\n\u001b[0;32m     28\u001b[0m lemmatized_tokens \u001b[39m=\u001b[39m [lemmatizer\u001b[39m.\u001b[39mlemmatize(token, pos\u001b[39m=\u001b[39mget_wordnet_pos(tag)) \u001b[39mfor\u001b[39;00m token, tag \u001b[39min\u001b[39;00m tagged_tokens]\n\u001b[0;32m     30\u001b[0m \u001b[39m# Identify named entities using NLTK's named entity recognition (NER) module\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MoRevolution\\miniconda3\\envs\\minimal_ds\\Lib\\site-packages\\nltk\\corpus\\util.py:124\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, attr)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'WordNetCorpusReader' object has no attribute 'WordNetLemmatizer'"
     ]
    }
   ],
   "source": [
    "#download using nltk.download('punkt') if you get an nltk error\n",
    "all_keywords = []\n",
    "for index, story in enumerate(corpus):\n",
    "    keywords = extract_keywords(story[index])\n",
    "    all_keywords.append(keywords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the keywords and phrases that are likely to appear in relevant stories\n",
    "relevant_keywords = ['low socio-economic background', 'higher education', 'difficulties', 'overcome']\n",
    "\n",
    "# Define the path to the file containing the stories\n",
    "file_path = 'path/to/file.csv'\n",
    "\n",
    "# Read in the stories from the file\n",
    "with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    stories = [row['story'] for row in csv_reader]\n",
    "\n",
    "# Define a vectorizer that will convert the stories into a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True)\n",
    "\n",
    "# Convert the stories into a matrix of TF-IDF features\n",
    "story_matrix = vectorizer.fit_transform(stories)\n",
    "\n",
    "# Loop through the stories and calculate the cosine similarity between each story and the relevant keywords\n",
    "for i, story in enumerate(stories):\n",
    "    # Convert the story into a matrix of TF-IDF features\n",
    "    story_vec = vectorizer.transform([story])\n",
    "\n",
    "    # Calculate the cosine similarity between the story and the relevant keywords\n",
    "    similarity = cosine_similarity(story_vec, story_matrix[:, [vectorizer.vocabulary_.get(word) for word in relevant_keywords]])\n",
    "\n",
    "    # If the similarity is above a certain threshold, print the story\n",
    "   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple classifer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
