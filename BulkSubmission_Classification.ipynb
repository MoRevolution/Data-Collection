{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f) \n",
    "    return (data)\n",
    "\n",
    "corpus = load_data(\"local_data.json\")[\"File\"]\n",
    "# corpus[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "# import zstandard as zstd\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressed_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\"\n",
    "# temp_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\temp\\RS_2005-06.zst\"\n",
    "decompressed_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\\Decompressed\\RS_2018-06.json\"\n",
    "output_path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\\Decompressed\\chunks\"\n",
    "# listing = glob.glob(temp_path + '\\\\*.zst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size=50000\n",
    "batch_no=1 \n",
    "\n",
    "for chunk in pd.read_json(decompressed_path,lines=True,chunksize=chunk_size):\n",
    "    chunk.to_csv(os.path.join(output_path, str(batch_no)+'.csv'),index=False)\n",
    "    batch_no+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\MoRevolution\\Desktop\\College\\Data Dump\\Reddit Sumbission Dump\\reddit\\submissions\\Decompressed\\chunks\\1.csv\"\n",
    "df = pd.read_csv(path)\n",
    "arr = np.array([\"college\", \"AskReddit\", \"science\", \"psychology\", \"socialwork\", \"personalfinance\", \"financialaid\"])\n",
    "count = (df['subreddit'].isin(arr)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['subreddit'].value_counts()[arr]\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g_story = []\n",
    "# Define the keywords and phrases \n",
    "relevant_keywords = ['low socio-economic background', 'higher education', 'difficulties', 'overcome']\n",
    "\n",
    "# Define the keywords and phrases that are likely to appear in irrelevant stories\n",
    "irrelevant_keywords = ['high-achieving', 'wealthy', 'privileged']\n",
    "\n",
    "\n",
    "for story in corpus: \n",
    "    # Check if the story contains any of the relevant keywords\n",
    "    relevant_count = sum(1 for keyword in relevant_keywords if keyword in story.lower())\n",
    "\n",
    "    # Check if the story contains any of the irrelevant keywords\n",
    "    irrelevant_count = sum(1 for keyword in irrelevant_keywords if keyword in story.lower())\n",
    "\n",
    "    # If the story is relevant, print it\n",
    "    if relevant_count > 0 and irrelevant_count == 0:\n",
    "        g_story.append(story)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = corpus\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = [data['text'] for data in labeled_data if data['label'] == 'relevant']\n",
    "train_labels = [data['label'] for data in labeled_data if data['label'] == 'relevant']\n",
    "test_data = [data['text'] for data in labeled_data if data['label'] != 'relevant']\n",
    "test_labels = [data['label'] for data in labeled_data if data['label'] != 'relevant']\n",
    "\n",
    "# Convert the text data into a bag-of-words representation\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "train_data_counts = vectorizer.fit_transform(train_data)\n",
    "test_data_counts = vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes classifier on the labeled data\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_data_counts, train_labels)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "predictions = clf.predict(test_data_counts)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Filter the relevant stories from the corpus\n",
    "with open('corpus.json', 'r') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "relevant_stories = []\n",
    "for story in corpus:\n",
    "    story_counts = vectorizer.transform([story['text']])\n",
    "    if clf.predict(story_counts) == 'relevant':\n",
    "        relevant_stories.append(story)\n",
    "\n",
    "print(f\"Found {len(relevant_stories)} relevant stories.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
